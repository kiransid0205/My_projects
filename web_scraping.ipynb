{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping data from Daft.ie and putting together a dataset for House rent predictions:\n",
    "\n",
    "#This program contains 2 parts:\n",
    "#Part I: Web-scraping from daft.ie.\n",
    "#Part II: Combining the data scraped to form a single dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part I: Web-scraping of data from daft.ie.\n",
    "\n",
    "#Step 1: Importing the necessary data from daft.ie\n",
    "import requests\n",
    "import bs4\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from requests import get\n",
    "from bs4 import BeautifulSoup\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 2: Creating a headers request with a 'user-agent' string which acts as an ID card containing basic information about the web page (version and type), for optimal performance and visuals.\n",
    "headers = {\n",
    "    'User-Agent': \"Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/86.0.4240.198 Safari/537.36\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "161"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Step 3: Extracting the number of pages of data to scrape from daft.ie.\n",
    "divs = np.arange(0, 3201, 20)\n",
    "divs = divs.astype(int)\n",
    "print(len(divs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 4: The scraping process! Extracting the addresses, rents, and accomodation type, and storing them all in lists:\n",
    "Initial_page = \"https://www.daft.ie/property-for-rent/dublin?pageSize=20&from=\" # the very first page\n",
    "\n",
    "#Initialising all the necessary lists. Each list is a nested list containing \n",
    "rates = [[]*len(divs)]  #nested list for rents\n",
    "adds = [[]*len(divs)]  #nested list for addresses\n",
    "houses = [[]*len(divs)]  #nested list for housing types\n",
    "beds = [[]*len(divs)]  #nested list for number of bedrooms\n",
    "baths = [[]*len(divs)]  #nested list for number of bathrooms\n",
    "mix = [[]*len(divs)]  #nested list for the first 4 pages' data of beds + baths + housing_type\n",
    "\n",
    "#a for loop running across all the pages:\n",
    "for i in divs:\n",
    "    url = Initial_page+str(i) # construct the url by pasting\n",
    "    r = requests.get(url,{'headers':headers})\n",
    "    soup = bs4.BeautifulSoup(r.text,'html.parser')\n",
    "    \n",
    "    #Scraping the rent data into 'rates'\n",
    "    a = soup.find_all('p', {'class': 'SubUnit__Title-sc-10x486s-5 keXaVZ', 'data-testid': 'sub-title'})\n",
    "    b = soup.find_all('span',{'class': 'TitleBlock__StyledSpan-sc-1avkvav-4 gDBFnc'})\n",
    "    rates.append(a)\n",
    "    rates.append(b)\n",
    "    \n",
    "    #Scraping the addresses into 'adds'\n",
    "    c = soup.find_all('p',{'class': 'TitleBlock__Address-sc-1avkvav-7 eARcqq', 'data-testid': 'address'})\n",
    "    d = soup.find_all('p',{'class': 'TitleBlock__Address-sc-1avkvav-7 knPImU', 'data-testid': 'address'})\n",
    "    adds.append(c)\n",
    "    adds.append(d)\n",
    "    \n",
    "    #Scraping the house specifications:\n",
    "    #1. Bedroom specs into beds\n",
    "    e = soup.find_all('p',{'class': 'TitleBlock__CardInfoItem-sc-1avkvav-8 jBZmlN', 'data-testid': 'beds'})\n",
    "    beds.append(e)\n",
    "    \n",
    "    #2. Bathroom specs into baths\n",
    "    f = soup.find_all('p',{'class': 'TitleBlock__CardInfoItem-sc-1avkvav-8 jBZmlN', 'data-testid': 'baths'})\n",
    "    baths.append(f)\n",
    "    \n",
    "    #3. Accomodation type into houses\n",
    "    g = soup.find_all('p', {'class': 'TitleBlock__CardInfoItem-sc-1avkvav-8 bcaKbv', 'data-testid': 'property-type'})\n",
    "    houses.append(g)\n",
    "    \n",
    "    #4. For the entries from pages 1-4, beds, baths and house_types are all combined:\n",
    "    h = soup.find_all('p', {'class': 'SubUnit__CardInfoItem-sc-10x486s-7 ftNycI'})\n",
    "    mix.append(h)\n",
    "    \n",
    "#The initial pages of the web-site (1-4, to be exact) had different code for the content uploaded. Hence, a bit of complication while extracting. However, to overcome this, I have used Python as well as Excel to filter and obtain the best data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3298 3139 2864 2863 3056 318\n"
     ]
    }
   ],
   "source": [
    "#Step 5: Converting each nested list to a regular list:\n",
    "rates2 = reduce(lambda x1,y1: x1+y1, rates)\n",
    "adds2 = reduce(lambda x2,y2: x2+y2, adds)\n",
    "beds2 = reduce(lambda x3,y3: x3+y3, beds)\n",
    "baths2 = reduce(lambda x4,y4: x4+y4, baths)\n",
    "houses2 = reduce(lambda x5,y5: x5+y5, houses)\n",
    "mix2 = reduce(lambda x6,y6: x6+y6, mix)\n",
    "\n",
    "#let's observe the number of elements in each list:\n",
    "#print(len(rates2), len(adds2), len(beds2), len(baths2), len(houses2), len(mix2))\n",
    "\n",
    "#the list of rents seems to have unusually high number of values, let's have a look why:\n",
    "#rates2[0:300]\n",
    "#we can observe that the title of the accomodation has been extracted as well. We need to filter them out and keep only the rents' values in the list.\n",
    "\n",
    "#Note: the 'print()' and 'rates2' commands were run to check some necessary details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of filtered rent dataset 3217\n",
      "Length of new filtered rent dataset 3215\n"
     ]
    }
   ],
   "source": [
    "#Step 6: initialising a new list to store irregular elements (which don't give information regarding the rent) and then removing them from the original list:\n",
    "extras = []   #empty list\n",
    "s = 0    #counter\n",
    "\n",
    "#for loop to go through the list containing web-scraped single list:\n",
    "for i in range(len(rates2)):\n",
    "    if '€' not in str(rates2[i]):    #checking if the symbol '€' exists in the information tag\n",
    "        extras.append(rates2[i])\n",
    "        s += 1\n",
    "rates3 = [k for k in rates2 if k not in extras]\n",
    "print(\"Length of filtered rent dataset\", len(rates3))\n",
    "\n",
    "#initialising another empty list for further filtering:\n",
    "extras2 = []     #new empty list\n",
    "s = 0    #re-initialising the counter\n",
    "\n",
    "#for loop through the length of the filtered  list:\n",
    "for j in range(len(rates3)):\n",
    "    if 'from' in str(rates3[j]):      #checking for the term 'from' in the string\n",
    "        extras2.append(rates3[j])\n",
    "        s += 1\n",
    "    elif 'up' in str(rates3[j]):      #checking for the term 'up' in the string\n",
    "        extras2.append(rates3[j])\n",
    "        s += 1\n",
    "rates4 = [k for k in rates3 if k not in extras2]\n",
    "print(\"Length of new filtered rent dataset\", len(rates4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 7: initialising and appending empty lists to store the necessary text from the scraped data:\n",
    "types = []\n",
    "bedrooms = []\n",
    "bathrooms = []\n",
    "rents = []\n",
    "addresses = []\n",
    "mixed = []\n",
    "\n",
    "for container in houses2:\n",
    "    #x = container.text\n",
    "    types.append(container.text)\n",
    "\n",
    "for container in beds2:\n",
    "    #y = container.text\n",
    "    bedrooms.append(container.text)\n",
    "    \n",
    "for container in baths2:\n",
    "    #z = container.text\n",
    "    bathrooms.append(container.text)\n",
    "\n",
    "for container in rates4:\n",
    "    #a = container.text\n",
    "    rents.append(container.text)\n",
    "\n",
    "for container in adds2:\n",
    "    addresses.append(container.text)\n",
    "    \n",
    "for container in mix2:\n",
    "    mixed.append(container.text)\n",
    "    \n",
    "#we do observe empty elements in the mixed list. Filtering them further to obtain all the non-null elements:\n",
    "mixed2 = []\n",
    "for i in range(len(mixed)):\n",
    "    if mixed[i]:\n",
    "        mixed2.append(mixed[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 8: Since the arrays are of different length, I'm exporting the data separately into 6 different dataframes.\n",
    "#The 6 different dataframes will be combined into one dataset using Excel.\n",
    "#The last dataframe \"Mixed-data\" is more complex, and contains data of bedrooms, bathrooms and accomodation type for the first few pages in daft.ie.\n",
    "df1 = pd.DataFrame({'House-Rent': rents})\n",
    "df2 = pd.DataFrame({'Bedrooms': bedrooms})\n",
    "df3 = pd.DataFrame({'Bathrooms': bathrooms})\n",
    "df4 = pd.DataFrame({'Housing-type': types})\n",
    "df5 = pd.DataFrame({'Address': addresses})\n",
    "df6 = pd.DataFrame({'Mixed-data': mixed2})\n",
    "\n",
    "df1.to_csv('House-rent.csv')\n",
    "df2.to_csv('Bedrooms.csv')\n",
    "df3.to_csv('Bathrooms.csv')\n",
    "df4.to_csv('Housing-type.csv')\n",
    "df5.to_csv('Address.csv')\n",
    "df6.to_csv('Mixed-data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part II: Combining the data scraped to form a single dataset.\n",
    "\n",
    "#Step 9: Import the dataset. Out dataset will be stored in a variable called HRdata:\n",
    "HRdata = pd.read_csv(\"house-rent_data.csv\")\n",
    "HRdata.head()\n",
    "\n",
    "#The dataset is loaded perfectly! Our variables are:\n",
    "#1. House-rent: € per month.\n",
    "#2. Bedrooms: No. of bedrooms.\n",
    "#3. Bathrooms: No. of bathrooms.\n",
    "#4. Housing-type: Type of housing --> Apartment/House/Studio.\n",
    "#5. Address: The physical location of the accomodation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 10: The 'Address' variable is complex and is difficult to use directly. Hence it would be best to get the postal code of each accomodation, which would provide more insight into the regression procedure later on.\n",
    "\n",
    "y = []    #empty list to store the postal codes\n",
    "\n",
    "#Dublin is split into 22 different postal codes, plus some additional regions which do not follow the mainstream postal code terminology. Hence, we shall compile a list of the localities manually (it took me 90 minutes to come up with the list and cross-check for accuracy).\n",
    "#comprehensive lists containing all the localities grouped under each individual postal code:\n",
    "dub1 = [\"IFSC\", \"Abbey Street\", \"Amiens Street\", \"Capel Street\", \"Dorset Street\", \"Henry Street\", \"Mary Street\", \"Mountjoy Square\", \"Marlborough Street\", \"North Wall\", \"O'Connell Street\", \"Parnell Square\", \"Talbot Street\"]\n",
    "dub2 = [\"Merrion Square\", \"Trinity College\", \"Temple Bar\", \"Grafton Street\", \"Stephen's Green\", \"Dame Street\", \"Leeson Street\", \"Grand Canal Dock\", \"City Quay\", \"Leinster House\", \"Mansion House\", \"Aungier Street\", \"Wexford Street\", \"Camden Street\", \"Baggot Street\", \"College Green\", \"Fitzwilliam Square\", \"Harcourt Street\", \"Kildare Street\", \"Lord Edward Street\", \"Mount Street\", \"Nassau Street\", \"Pearse Street\", \"Georges Street\", \"Hanover Quay\"]\n",
    "dub3 = [\"Ballybough\", \"North Strand\", \"Clonliffe\", \"Cloniffe\", \"Clontarf\", \"Dollymount\", \"East Wall\", \"East Point\", \"Fairview\", \"Killester\", \"Marino\"]\n",
    "dub4 = [\"Ballsbridge\", \"Belfield\", \"Donnybrook\", \"Irishtown\", \"Merrion\", \"Pembroke\", \"Ringsend\", \"Sandymount\", \"RDS\", \"Lansdowne Road\"]\n",
    "dub5 = [\"Artane\", \"Harmonstown\", \"Kilbarrack\", \"Raheny\"]\n",
    "dub6 = [\"Milltown\", \"Ranelagh\", \"Rathmines\", \"Dartry\", \"Rathgar\"]\n",
    "dub6w = [\"Harold's Cross\", \"Templeogue\", \"Kimmage\", \"Terenure\"]\n",
    "dub7 = [\"Arbour Hill\", \"Ashtown\", \"Broadstone\", \"Cabra\", \"North Circular Road\", \"Grangegorman\", \"Oxmantown\", \"Phibsboro\", \"Smithfield\", \"Stoneybatter\", \"Four Courts\", \"Phibsborough\", \"Navan Road\", \"Navan road\", \"Ashington\"]\n",
    "dub8 = [\"Dolphin's Barn\", \"Inchicore\", \"Islandbridge\", \"Kilmainham\", \"Merchants Quay\", \"Portobello\", \"South Circular Road\", \"Phoenix Park\", \"Liberties\", \"Christchurch\", \"St. Patrick's Cathedral\", \"Coombe\", \"Island Bridge\", \"Rialto\"]\n",
    "dub9 = [\"Kilmore\", \"Shangan\", \"Coultry\", \"Beaumont\", \"Donnycarney\", \"Drumcondra\", \"Elm Mount\", \"Griffith Avenue\", \"Glasnevin\", \"St Mobhi\", \"Botanic Gardens\", \"Santry\", \"Whitehall\"]\n",
    "dub10 = [\"Ballyfermot\", \"Sarsfield Road\", \"Cherry Orchard\"]\n",
    "dub11 = [\"Ballymun\", \"Sillogue\", \"Balcurris\", \"Balbutcher\", \"Poppintree\", \"Sandyhill\", \"Wadelai\", \"Dubber\", \"Finglas\", \"Ballygal\", \"Cappagh\", \"Glasnevin\", \"Cremore\", \"Addison\", \"Violet Hill\", \"Finglas Road\", \"Old Finglas Road\", \"Glasnevin Cemetery\", \"Kilshane\", \"The Ward\", \"Coolquay\", \"Jamestown\"]\n",
    "dub12 = [\"Bluebell\", \"Crumlin\", \"Drimnagh\", \"Greenhills\", \"Perrystown\", \"Walkinstown\"]\n",
    "dub13 = [\"Clarehall\", \"Baldoyle\", \"Bayside\", \"Donaghmede\", \"Clongriffin\", \"Sutton\", \"Howth\", \"Ayrfield\", \"Balgriffin\", \"Portmarnock\"]\n",
    "dub14 = [\"Churchtown\", \"Clonskeagh\", \"Dundrum\", \"Goatstown\", \"Windy Arbour\", \"Roebuck\"]\n",
    "dub15 = [\"Hollystown\", \"Ashtown\", \"Blanchardstown\", \"Castleknock\", \"Coolmine\", \"Clonsilla\", \"Corduff\", \"Mulhuddart\", \"Tyrrelstown\", \"Clonee\", \"Ongar\", \"Carpenterstown\"]\n",
    "dub16 = [\"Ballinteer\", \"Ballyboden\", \"Dundrum\", \"Kilmashogue\", \"Knocklyon\", \"Rathfarnham\", \"Rockbrook\"]\n",
    "dub17 = [\"Balgriffin\", \"Coolock\", \"Belcamp\", \"Darndale\", \"Priorswood\", \"Riverside\", \"Clonshaugh\"]\n",
    "dub18 = [\"Rathmichael\", \"Cherrywood\", \"Cabinteely\", \"Carrickmines\", \"Foxrock\", \"Kilternan\", \"Sandyford\", \"Shankill\", \"Ticknock\", \"Ballyedmonduff\", \"Stepaside\", \"Leopardstown\"]\n",
    "dub20 = [\"Chapelizod\", \"Palmerstown\"]\n",
    "dub22 = [\"Park West\", \"Clondalkin\", \"Rowlagh\", \"Quarryvale\", \"Liffey Valley\", \"Neilstown\", \"Bawnogue\", \"Kingswood\"]\n",
    "dub24 = [\"Firhouse\", \"Jobstown\", \"Old Bawn\", \"Oldbawn\", \"Kilnamanagh\", \"Tallaght\", \"Saggart\", \"Citywest\", \"Aylesbury\", \"Rathcoole\"]\n",
    "ext1 = [\"Blackrock\", \"Deansgrange\", \"Monkstown\", \"Stillorgan\", \"Booterstown\", \"Kilmacud\"]\n",
    "ext2 = [\"Dun Laoghaire\", \"Dalkey\", \"Glasthule\", \"Glenageary\", \"Sandycove\", \"Killiney\"]\n",
    "ext3 = [\"Swords\", \"Kilsallaghan\", \"Lusk\"]\n",
    "ext4 = [\"Malahide\", \"Donabate\"]\n",
    "ext5 = [\"Lucan\", \"Adamstown\"]\n",
    "ext6 = [\"Rush\"]\n",
    "ext7 = [\"Balbriggan\"]\n",
    "\n",
    "#for loop to assign the proper pincode to each address. I have used the lists created above for this.\n",
    "for i in range(len(adds)):\n",
    "    #since some addresses do have the postal codes mentioned, I have used a simple regex code to extract the code directly:\n",
    "    x = re.findall(r'Dublin [0-9]+', adds[i])\n",
    "    if x:\n",
    "        y.append(str(x))\n",
    "    #for the addresses where the postal code is not mentioned:\n",
    "    else:\n",
    "        if any(x in adds[i] for x in dub1):\n",
    "            y.append('Dublin 1')\n",
    "        elif any(x in adds[i] for x in dub2):\n",
    "            y.append('Dublin 2')\n",
    "        elif any(x in adds[i] for x in dub3):\n",
    "            y.append('Dublin 3')\n",
    "        elif any(x in adds[i] for x in dub4):\n",
    "            y.append('Dublin 4')\n",
    "        elif any(x in adds[i] for x in dub5):\n",
    "            y.append('Dublin 5')\n",
    "        elif any(x in adds[i] for x in dub6):\n",
    "            y.append('Dublin 6')\n",
    "        elif any(x in adds[i] for x in dub6w):\n",
    "            y.append('Dublin 6W')\n",
    "        elif any(x in adds[i] for x in dub7):\n",
    "            y.append('Dublin 7')\n",
    "        elif any(x in adds[i] for x in dub8):\n",
    "            y.append('Dublin 8')\n",
    "        elif any(x in adds[i] for x in dub9):\n",
    "            y.append('Dublin 9')\n",
    "        elif any(x in adds[i] for x in dub10):\n",
    "            y.append('Dublin 10')\n",
    "        elif any(x in adds[i] for x in dub11):\n",
    "            y.append('Dublin 11')\n",
    "        elif any(x in adds[i] for x in dub12):\n",
    "            y.append('Dublin 12')\n",
    "        elif any(x in adds[i] for x in dub13):\n",
    "            y.append('Dublin 13')\n",
    "        elif any(x in adds[i] for x in dub14):\n",
    "            y.append('Dublin 14')\n",
    "        elif any(x in adds[i] for x in dub15):\n",
    "            y.append('Dublin 15')\n",
    "        elif any(x in adds[i] for x in dub16):\n",
    "            y.append('Dublin 16')\n",
    "        elif any(x in adds[i] for x in dub2):\n",
    "            y.append('Dublin 17')\n",
    "        elif any(x in adds[i] for x in dub18):\n",
    "            y.append('Dublin 18')\n",
    "        elif any(x in adds[i] for x in dub20):\n",
    "            y.append('Dublin 20')\n",
    "        elif any(x in adds[i] for x in dub22):\n",
    "            y.append('Dublin 22')\n",
    "        elif any(x in adds[i] for x in dub24):\n",
    "            y.append('Dublin 24')\n",
    "        elif any(x in adds[i] for x in ext1):\n",
    "            y.append('A94')\n",
    "        elif any(x in adds[i] for x in ext2):\n",
    "            y.append('A96')\n",
    "        elif any(x in adds[i] for x in ext3):\n",
    "            y.append('K67')\n",
    "        elif any(x in adds[i] for x in ext4):\n",
    "            y.append('K36')\n",
    "        elif any(x in adds[i] for x in ext5):\n",
    "            y.append('K78')\n",
    "        elif any(x in adds[i] for x in ext6):\n",
    "            y.append('K56')\n",
    "        elif any(x in adds[i] for x in ext7):\n",
    "            y.append('K32')\n",
    "        else:\n",
    "            y.append('NA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 11: Assigning the Postal-code list to the HRdata dataframe.\n",
    "HRdata['Postal-code'] = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 12: Exporting the dataset to the working directory.\n",
    "HRdata.to_csv(\"HRdata.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
